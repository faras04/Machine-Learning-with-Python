{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COMPARATIVE STUDY OF DIFFERENT MACHINE LEARNING MODELS FOR SALES PREDICTION AND FRAUD DETECTION."},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"In recent years, the rise of the Internet of things (IoT) as an emerging technology has been unbelievable, more companies are moving towards the adoption of these technologies and many IoT sensors are being deployed to share information in real-time which leads to the generation of a huge amount of data. This data when used correctly, will be very helpful to the company to discover hidden patterns for better decision making in the future. For example, with the DataCo company, dataset customer segmentation analysis was performed in this project which helps the company to better understand its customers and target them to increase customer responsiveness and the company's revenue. With a lot of options available to analyze data, it is very difficult to decide which method and machine learning model to use since the performance of the model vary on the parameters available in the data. \n\nWith the growth of machine learning, there have been numerous comparison studies that compare the performance of neural networks with traditional linear techniques for forecasting. For example, author Carbonneau et al. (2007) in his research work compared various traditional forecasting time-series like moving average, linear regression with recurrent neural networks and support vector machines and concluded that recurrent neural networks performed best. Hill et al. (1996) have also considered the M-competition data and have compared between neural networks and traditional methods. Vakili et al. (2020) evaluated the performance of 11 popular machine and deep learning algorithms for classification task using six IoT-related datasets and concluded that Random Forests performed better than other machine learning models, while among deep learning models, ANN and CNN achieved more interesting results. Some other authors like Ahmed et al. (2010) did study comparing different regression models and concluded that the MLP model and Gaussian process models are the best two models for regression type data. But no study that compared both Classification type ML models and Regression type ML models against the Neural Network models with the same dataset was found.\n\nThis project aims to compare 9 popular machine learning classifiers and 7 regressors type machine learning models and measure their performance against neural network models to find out which machine learning model performs better. Since the dataset used is related to supply chain important parameters are identified and the machine learning models are trained with the dataset for detection of fraud transactions, late delivery of orders, sales revenue and quantity of products which customer orders. The machine learning classifiers used in this project are Logistic Regression,Linear Discriminant Analysis, Gaussian Naive Bayes, Support Vector Machines, k - Nearest Neighbors, Random Forest classification, Extra Trees classification, Extreme Gradient Boosting, Decision Tree classification for fraud detection and to predict late delivery on the basis accuracy, recall score and F1 score. The regression models used are Lasso, Ridge, Light Gradient boosting, Random Forest regression, Extreme Gradient Boosting regression, Decision Tree Regression, and Linear Regression to predict sales and quantity of the products required which are compared with mean absolute error (MAE) and root mean square error (RMSE)."},{"metadata":{},"cell_type":"markdown","source":"# Data Collection"},{"metadata":{},"cell_type":"markdown","source":"The dataset used in this project is maintained transparently with the Creative Commons 4.0 license by Fabian Constante, Fernando Silva, and António Pereira through the Mendeley data repository. The dataset consists of roughly 180k transactions from supply chains used by the company DataCo Global for 3 years. The dataset can be downloaded from:\n\nhttps://data.mendeley.com/datasets/8gx2fvg2k6/5"},{"metadata":{},"cell_type":"markdown","source":"### Importing all required libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nimport datetime as dt\nimport calendar,warnings,itertools,matplotlib,keras,shutil\nimport tensorflow as tf\nimport statsmodels.api as sm\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict\nfrom sklearn import svm,metrics,tree,preprocessing,linear_model\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge,LinearRegression,LogisticRegression,ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier, GradientBoostingRegressor,BaggingClassifier,ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score,mean_squared_error,recall_score,confusion_matrix,f1_score,roc_curve, auc\nfrom sklearn.datasets import load_iris,make_regression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.kernel_ridge import KernelRidge\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom IPython.core import display as ICD\nfrom tensorflow_core.estimator import inputs\n\n#Hiding the warnings\nwarnings.filterwarnings('ignore') \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing Dataset using pandas\ndataset=pd.read_csv(\"../input/dataco-smart-supply-chain-for-big-data-analysis/DataCoSupplyChainDataset.csv\",header= 0,encoding= 'unicode_escape')\ndataset.head(5)# Checking 5 rows in dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The total data set consists of 180519 records and 53 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.apply(lambda x: sum(x.isnull())) #Checking missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data consists of some missing values from Customer Lname, Product Description, Order Zipcode and, Customer Zipcode which should be removed or replaced before proceeding with the analysis. And also, since there is a chance different customers might have the same first name or same last name a new column with ‘customer full name’ is created to avoid any ambiguities."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding first name and last name together to create new column\ndataset['Customer Full Name'] = dataset['Customer Fname'].astype(str)+dataset['Customer Lname'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make it easier for analysis some unimportant columns are dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=dataset.drop(['Customer Email','Product Status','Customer Password','Customer Street','Customer Fname','Customer Lname',\n           'Latitude','Longitude','Product Description','Product Image','Order Zipcode','shipping date (DateOrders)'],axis=1)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThere are 3 missing values in Customer Zipcode column. Since the missing values are just zip codes which are not very important these are replaced with zero before proceeding with data analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Customer Zipcode']=data['Customer Zipcode'].fillna(0)#Filling NaN columns with zero","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualisation"},{"metadata":{},"cell_type":"markdown","source":"To find important parameters, data correlation is performed."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(24,12))         # figsize\nsns.heatmap(data.corr(),annot=True,linewidths=.5,fmt='.1g',cmap= 'coolwarm') # Heatmap for correlation matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that product price price has high correlation with Sales,Order Item Total."},{"metadata":{},"cell_type":"markdown","source":"As the data which is being used for analysis is related to Supply chain, it makes sense to find which region has most sales? It can be found by using groupby method which will segregate similar market regions together and add all sales for that particular region using 'sum' function."},{"metadata":{"trusted":true},"cell_type":"code","source":"market = data.groupby('Market') #Grouping by market\nregion = data.groupby('Order Region')\nplt.figure(1)\nmarket['Sales per customer'].sum().sort_values(ascending=False).plot.bar(figsize=(12,6), title=\"Total sales for all markets\")\nplt.figure(2)\nregion['Sales per customer'].sum().sort_values(ascending=False).plot.bar(figsize=(12,6), title=\"Total sales for all regions\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It could be seen from the graph that European market has the most number of sales whereas Africa has the least.In these markets western europe regions and central america recorded highest sales. \n\nWhich catergory of products has highest sales?The same method can be followed here to see the product category with highest sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grouping all categories\ncat = data.groupby('Category Name')\nplt.figure(1)\n# Total sum of sales for all categories\ncat['Sales per customer'].sum().sort_values(ascending=False).plot.bar(figsize=(12,6), title=\"Total sales\")\n# Mean sales for all categories\nplt.figure(2)\ncat['Sales per customer'].mean().sort_values(ascending=False).plot.bar(figsize=(12,6), title=\"Average sales\")\nplt.figure(3)\n# Mean prices for all categories\ncat['Product Price'].mean().sort_values(ascending=False).plot.bar(figsize=(12,6), title=\"Average price\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from fig 1 that the fishing category had most number of sales followed by the Cleats.However it is suprising to see that top 7 products with highest price on average are the most sold products on average with computers having almost 1350 sales despite price being 1500$. Since correlation was high between Price and Sales it will be intresting to see how price is impacting the sales for all the products to see the trend."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.plot(x='Product Price', y='Sales per customer',linestyle='dotted',\n     markerfacecolor='blue', markersize=12) \nplt.title('Product Price vs Sales per customer')#title\nplt.xlabel('Product Price')  # X-axis title\nplt.ylabel('Sales per customer') # Y=axis title\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be observed that prices has linear relation with sales. Which quarter recorded highest sales? It can be found  by dividing order time into years,months,week day,hour to better observe the trend."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata['order_year']= pd.DatetimeIndex(data['order date (DateOrders)']).year\ndata['order_month'] = pd.DatetimeIndex(data['order date (DateOrders)']).month\ndata['order_week_day'] = pd.DatetimeIndex(data['order date (DateOrders)']).weekday_name\ndata['order_hour'] = pd.DatetimeIndex(data['order date (DateOrders)']).hour\ndata['order_month_year'] = pd.to_datetime(data['order date (DateOrders)']).dt.to_period('M')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quater= data.groupby('order_month_year')\nquartersales=quater['Sales'].sum().resample('Q').mean().plot(figsize=(15,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By seeing above graph it can be seen that sales are consistent from Q1 2015 until Q3 of 2017 and suddenly dipped by Q1 2018. What is the purchase trend in week days,hours and months?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,12))\nplt.subplot(4, 2, 1)\nquater= data.groupby('order_year')\nquater['Sales'].mean().plot(figsize=(12,12),title='Average sales for 3 years')\nplt.subplot(4, 2, 2)\ndays=data.groupby(\"order_week_day\")\ndays['Sales'].mean().plot(figsize=(12,12),title='Average sales per week in days')\nplt.subplot(4, 2, 3)\nhrs=data.groupby(\"order_hour\")\nhrs['Sales'].mean().plot(figsize=(12,12),title='Average sales per day in hrs')\nplt.subplot(4, 2, 4)\nmnth=data.groupby(\"order_month\")\nmnth['Sales'].mean().plot(figsize=(12,12),title='Average sales per year in mnths')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How price is impacting sales, when and which products are having more sales are found.The most number of orders came in October followed by November, and orders for all other months are consistent.Highest number of orders are placed by customers in 2017. \nSaturday recorded highest number of average sales and wednesday with the least number of sales. The average sales are consistent throughout the day irrespective of time with std of 3."},{"metadata":{},"cell_type":"markdown","source":"It is also important to know what type of payment method is being preferred by people to buy all these products in all regions? It can be found using .unique() method to see different payment methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Type'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is found that four types of payment methods are used.Which payment method is preferred the most by people in different regions?"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#xyz = data.groupby('Type')\nxyz1 = data[(data['Type'] == 'TRANSFER')]\nxyz2= data[(data['Type'] == 'CASH')]\nxyz3= data[(data['Type'] == 'PAYMENT')]\nxyz4= data[(data['Type'] == 'DEBIT')]\ncount1=xyz1['Order Region'].value_counts()\ncount2=xyz2['Order Region'].value_counts()\ncount3=xyz3['Order Region'].value_counts()\ncount4=xyz4['Order Region'].value_counts()\nnames=data['Order Region'].value_counts().keys()\nn_groups=23\nfig,ax = plt.subplots(figsize=(20,8))\nindex=np.arange(n_groups)\nbar_width=0.2\nopacity=0.6\ntype1=plt.bar(index,count1,bar_width,alpha=opacity,color='b',label='Transfer')\ntype2=plt.bar(index+bar_width,count2,bar_width,alpha=opacity,color='r',label='Cash')\ntype3=plt.bar(index+bar_width+bar_width,count3,bar_width,alpha=opacity,color='g',label='Payment')\ntype4=plt.bar(index+bar_width+bar_width+bar_width,count4,bar_width,alpha=opacity,color='y',label='Debit')\nplt.xlabel('Order Regions')\nplt.ylabel('Number of payments')\nplt.title('Different Type of payments used in all regions')\nplt.legend()\nplt.xticks(index+bar_width,names,rotation=90)\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Debit type is most preferred payment method by people in all regions, Cash payment being the least preferred method."},{"metadata":{},"cell_type":"markdown","source":"Some products are having negative benefit per orders which indicates that the orders are generating loss of revenue to the company. Which products are these?"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = data[(data['Benefit per order']<0)]\n#Plotting top 10 products with most loss\nplt.figure(1)\nloss['Category Name'].value_counts().nlargest(10).plot.bar(figsize=(20,8), title=\"Products with most loss\")\nplt.figure(2)\nloss['Order Region'].value_counts().nlargest(10).plot.bar(figsize=(20,8), title=\"Regions with most loss\")\n#Sum of total sales which are lost\nprint('Total revenue lost with orders',loss['Benefit per order'].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The total loss sales are approximately 3.9 Millions which is an huge amount.It can be seen that Cleats is the category with most loss sales followed by Mens footwear.Most lost sales are happeing in central america and western europe region.This lost sales may have happened due to suspected frauds or late deliveries.\n\nFinding which payment method is used to conduct frauds can be useful to  prevent fraud from happening in future"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Checking type of payment used to conduct fraud other than Transfer\nxyz = data[(data['Type'] != 'TRANSFER')&(data['Order Status'] == 'SUSPECTED_FRAUD')]\nxyz['Order Region'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be clearly seen that there are no frauds conducted with DEBIT,CASH,PAYMENT methods so all the suspected fraud orders are made using wire transfer probably from abroad. Which region and what product is being suspected to the fraud the most? "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"high_fraud = data[(data['Order Status'] == 'SUSPECTED_FRAUD') & (data['Type'] == 'TRANSFER')]#separating orders with suspected fraud\n#Plotting pie chart with respect to order region\nfraud=high_fraud['Order Region'].value_counts().plot.pie(figsize=(24,12),\n                                                  startangle=180, explode=(0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),autopct='%.1f',shadow=True,)\nplt.title(\"Regions with Highest Fraud\",size=15,color='r') # Plotting title\nplt.ylabel(\" \")\nfraud.axis('equal') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be observed that highest number of suspected fraud orders are from Western Europe which is approximately 17.4% of total orders followed by Central America with 15.5%. Which product is being suspected fraud the most?"},{"metadata":{"trusted":true},"cell_type":"code","source":"high_fraud1 = data[(data['Order Status'] == 'SUSPECTED_FRAUD')] # \nhigh_fraud2 = data[(data['Order Status'] == 'SUSPECTED_FRAUD') &(data['Order Region'] == 'Western Europe')]\n#Plotting bar chart for top 10 most suspected fraud department in all regions\nfraud1=high_fraud1['Category Name'].value_counts().nlargest(10).plot.bar(figsize=(20,8), title=\"Fraud Category\",color='orange')\n#Plotting bar chart for top 10 most suspected fraud department in Western Europe\nfraud2=high_fraud2['Category Name'].value_counts().nlargest(10).plot.bar(figsize=(20,8), title=\"Fraud product in Western Europe\",color='green')\nplt.legend([\"All regions\", \"Western Europe\"])\nplt.title(\"Top 10 products with highest fraud detections\", size=15)\nplt.xlabel(\"Products\", size=13)\nplt.ylim(0,600)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is very suprising to see that cleats department is being suspected to fraud the most followed by Men's footwear in all the regions and also in Western Europe.Which customers are conducting all these fraud?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filtering out suspected fruad orders\ncus = data[(data['Order Status'] == 'SUSPECTED_FRAUD')] \n#Top 10 customers with most fraud\ncus['Customer Full Name'].value_counts().nlargest(10).plot.bar(figsize=(20,8), title=\"Top 10 Highest Fraud Customers\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The customer named Mary Smith alone was responible for trying to conduct fraud 528 times which is very shocking .How  much amount exactly did she conduct fraud orders?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filtering orders of mary smith with suspected fraud\namount = data[(data['Customer Full Name'] == 'MarySmith')&(data['Order Status'] == 'SUSPECTED_FRAUD')]\n#Plotting bar chart for top 10 most suspected fraud customers\namount['Sales'].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The total amount was almost 102k which is very huge amount.Since Mary was using different address every time when placing orders, a new customer id was issued each time which makes it difficult to identify the customer and ban them. All these parameters should be taken into consideration to improve fraud detection algorithm so fraud can be identified more accurately."},{"metadata":{},"cell_type":"markdown","source":"Delivering products to customer on time without late delivery is another important aspect for a supply chain company because customers will not be satisfied if products are not delivered on time. What  category of products are being delivered late the most?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filtering columns with late delivery status\nlate_delivery = data[(data['Delivery Status'] == 'Late delivery')]\n#Top 10 products with most late deliveries\nlate_delivery['Category Name'].value_counts().nlargest(10).plot.bar(figsize=(20,8), title=\"Top 10 products with most late deliveries\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that orders with Cleats department is getting delayed the most followed by Men's Footwear.For some orders risk of late delivery is given in data.The products with late delivery risk are compared with late delivered products. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filtering orders with late delivery risk\nxyz1 = data[(data['Late_delivery_risk'] == 1)]\n#Filtering late delivered orders\nxyz2 = data[(data['Delivery Status'] == 'Late delivery')]\ncount1=xyz1['Order Region'].value_counts()\ncount2=xyz2['Order Region'].value_counts()\n#Index names\nnames=data['Order Region'].value_counts().keys()\nn_groups=23\nfig,ax = plt.subplots(figsize=(20,8)) #Figure size\nindex=np.arange(n_groups)\nbar_width=0.2\nopacity=0.8\ntype1=plt.bar(index,count1,bar_width,alpha=opacity,color='r',label='Risk of late delivery')\ntype2=plt.bar(index+bar_width,count2,bar_width,alpha=opacity,color='y',label='Late delivery')\nplt.xlabel('Order Regions')\nplt.ylabel('Number of shipments')\nplt.title('Late delivered products used in all regions')\nplt.legend()\nplt.xticks(index+bar_width,names,rotation=90)\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus,it can be concluded that for all the products with late delivery risk irrespective of region the product is actually being delivered late,to avoid late delivery the company can ship products faster using better shipping method or schedule more time of Days for shipment so customers will know in advance when the products will reach them. It will be interesting to see the number of late deliveried orders for different types of shipment method in all regions."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filtering late delivery orders with standard class shipping\nxyz1 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Standard Class')]\n#Filtering late delivery orders with first class shipping\nxyz2 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'First Class')]\n#Filtering late delivery orders with second class shipping\nxyz3 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Second Class')]\n#Filtering late delivery orders with same day shipping\nxyz4 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Same Day')]\n#Counting total values\ncount1=xyz1['Order Region'].value_counts()\ncount2=xyz2['Order Region'].value_counts()\ncount3=xyz3['Order Region'].value_counts()\ncount4=xyz4['Order Region'].value_counts()\n#Index names\nnames=data['Order Region'].value_counts().keys()\nn_groups=23\nfig,ax = plt.subplots(figsize=(20,8))\nindex=np.arange(n_groups)\nbar_width=0.2\nopacity=0.6\ntype1=plt.bar(index,count1,bar_width,alpha=opacity,color='b',label='Standard Class')\ntype2=plt.bar(index+bar_width,count2,bar_width,alpha=opacity,color='r',label='First class')\ntype3=plt.bar(index+bar_width+bar_width,count3,bar_width,alpha=opacity,color='g',label='second class')\ntype4=plt.bar(index+bar_width+bar_width+bar_width,count4,bar_width,alpha=opacity,color='y',label='same day')\nplt.xlabel('Order Regions')\nplt.ylabel('Number of shipments')\nplt.title('Different Types of shipping methods used in all regions')\nplt.legend()\nplt.xticks(index+bar_width,names,rotation=90)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected the most number of late deliveries  for all regions occured with standard class shipping,with same day shipping being the one with least number of late deliveries.Both the first class and second class shipping have almost equal number of late deliveries."},{"metadata":{},"cell_type":"markdown","source":"# Customer Segmentation"},{"metadata":{},"cell_type":"markdown","source":"Understanding customer needs and targeting specific clusters of customers based on their need is one way for a supply chain company to increase number of customers and also to gain more profits.Since,purchase history of customers is already avaialble in the dataset, it can use RFM analysis for customer segmention. Even though there are so many different methods for customer segmentation,RFM analysis is being used because it utilizes numerical values to show Customer recency,frequency and monetary values and also the output results are easy to interpret."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating total price for which each order\ndata['TotalPrice'] = data['Order Item Quantity'] * data['Order Item Total']# Multiplying item price * Order quantity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['order date (DateOrders)'].max() # Calculating when the last order come to check recency","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last order in the dataset was made on 2018-01-31. So the present time is set slightly above than the last order time for more accuracy of recency value."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Present date was set to next day of the last order. i.e,2018-02-01\npresent = dt.datetime(2018,2,1)\ndata['order date (DateOrders)'] = pd.to_datetime(data['order date (DateOrders)'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping all values into new data frame named customer segmentation\nCustomer_seg = data.groupby('Order Customer Id').agg({'order date (DateOrders)': lambda x: (present - x.max()).days, 'Order Id': lambda x: len(x), 'TotalPrice': lambda x: x.sum()})\n#Changing order dates to int format\nCustomer_seg['order date (DateOrders)'] = Customer_seg['order date (DateOrders)'].astype(int)\n# Renaming columns as R_Value,F_Value,M_Value\nCustomer_seg.rename(columns={'order date (DateOrders)': 'R_Value', \n                         'Order Id': 'F_Value', \n                         'TotalPrice': 'M_Value'}, inplace=True)\nCustomer_seg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"R_Value(Recency) indicates how much time elapsed since a customer last order.\n\nF_Value(Frequency) indicates how many times a customer ordered.\n\nM_Value(Monetary value) tells us how much a customer has spent purchasing items."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10)) # Figure size\nplt.subplot(3, 1, 1)\nsns.distplot(Customer_seg['R_Value'])# Plot distribution of R_Value\nplt.subplot(3, 1, 2) \nsns.distplot(Customer_seg['F_Value'])# Plot distribution of F_Value\nplt.subplot(3, 1, 3)\nsns.distplot(Customer_seg['M_Value'])# Plot distribution of M_Value\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quantiles = Customer_seg.quantile(q=[0.25,0.5,0.75]) #Dividing RFM data into four quartiles\nquantiles = quantiles.to_dict() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The total data is divided into 4 quantiles. The R_Value should be low because it indicates recent customer activity and F_value, M_Value should be high since they indicate frequency and total value of purchase. Function is defined to indicate quantiles as numerical values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# R_Score should be minimum so 1st quantile is set as 1.\ndef R_Score(a,b,c):\n    if a <= c[b][0.25]:\n        return 1\n    elif a <= c[b][0.50]:\n        return 2\n    elif a <= c[b][0.75]: \n        return 3\n    else:\n        return 4\n# The higher the F_Score,M_Score the better so 1st quantile is set as 4.    \ndef FM_Score(x,y,z):\n    if x <= z[y][0.25]:\n        return 4\n    elif x <= z[y][0.50]:\n        return 3\n    elif x <= z[y][0.75]: \n        return 2\n    else:\n        return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New column for R_Score to indicate numerical score between 1 to 4.\nCustomer_seg['R_Score'] = Customer_seg['R_Value'].apply(R_Score, args=('R_Value',quantiles))\n# New column for F_Score to indicate numerical score between 1 to 4.\nCustomer_seg['F_Score'] = Customer_seg['F_Value'].apply(FM_Score, args=('F_Value',quantiles))\n# New column for M_Score to indicate numerical score between 1 to 4.\nCustomer_seg['M_Score'] = Customer_seg['M_Value'].apply(FM_Score, args=('M_Value',quantiles))\nCustomer_seg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The individual scores of R,F,M are known.A column for combined RFM score is created."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Adding R,F,M Scores to one new column\nCustomer_seg['RFM_Score'] = Customer_seg.R_Score.astype(str)+ Customer_seg.F_Score.astype(str) + Customer_seg.M_Score.astype(str)\nCustomer_seg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many different customer segments are there in total can be found using .unique() and len method."},{"metadata":{"trusted":true},"cell_type":"code","source":"count=Customer_seg['RFM_Score'].unique()\nprint(count)# Printing all Unique values\nlen(count)# Total count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that there are 33 different customer segments. To make it easier for segmentation individual R,F,M scores are added together"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate RFM_Score\nCustomer_seg['RFM_Total_Score'] = Customer_seg[['R_Score','F_Score','M_Score']].sum(axis=1)\nCustomer_seg['RFM_Total_Score'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 9 values in total for customer segmentation.Appropriate names were assigned for each value seperately."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define rfm_level function\ndef RFM_Total_Score(df):\n\n    if (df['RFM_Total_Score'] >= 11):# For RFM score with values 11,12\n        return 'Champions' \n    elif (df['RFM_Total_Score'] == 10):# For RFM score with value 10\n        return 'Loyal Customers' \n    elif (df['RFM_Total_Score'] == 9): # For RFM score with value 9\n        return 'Recent Customers'\n    elif (df['RFM_Total_Score'] == 8): # For RFM score with value 8\n         return 'Promising'\n    elif (df['RFM_Total_Score'] == 7): # For RFM score with value 7\n        return 'Customers Needing Attention'\n    elif (df['RFM_Total_Score'] == 6): # For RFM score with value 6\n        return 'Cant lose them'\n    elif (df['RFM_Total_Score'] == 5): # For RFM score with value 5\n        return 'At Risk'\n    else:                               # For RFM score with value less than 5\n        \n        return 'Lost'\n# Create a new variable RFM_Level\nCustomer_seg['Customer_Segmentation'] =Customer_seg.apply(RFM_Total_Score, axis=1)\n# Print the header with top 5 rows to the console\nCustomer_seg.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many customers are present in each segment?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate average values for each RFM_Level, and return a size of each segment \nCustomer_seg['Customer_Segmentation'].value_counts().plot.pie(figsize=(10,10),\n                                        startangle=135, explode=(0,0,0,0.1,0,0,0,0),autopct='%.1f',shadow=True)\nplt.title(\"Customer Segmentation\",size=15)\nplt.ylabel(\" \")\nplt.axis('equal') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since total customers are divided into 9 segments it can be seen that, 11.4% customers are at risk of losing them as customers and 11% customers needs attention else even they will be lost eventually.It can be seen that 4.4% of customers are already lost. "},{"metadata":{},"cell_type":"markdown","source":"Our Top 10 Churned best customers who has not purchased anything in a while"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"churned=Customer_seg[(Customer_seg['RFM_Score']=='411')].sort_values('M_Value', ascending=False).head(10)\nchurned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These customers used to place orders with huge amounts very frequently but they did not place orders from almost a year which means they are purchasing from other companies. These groups of people should be targeted with offers to gain them back."},{"metadata":{},"cell_type":"markdown","source":"Top 10 new best customers who place costly orders often."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#The R_Score should be low and F_Score, M_Score should be as high as possible\nCustomer_seg[(Customer_seg['RFM_Score']=='144')|(Customer_seg['RFM_Score']=='143')].sort_values('M_Value', ascending=False).head(10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above customers has the potential to become best customers this people should be targeted to convert them into loyal customers.All these different segment of customers should be targeted with different tailored advertisments and rewards for increased profits and more responsiveness from customers."},{"metadata":{},"cell_type":"markdown","source":"# Data Modelling"},{"metadata":{},"cell_type":"markdown","source":"To measure the performance of different models the machine learning models are trained to detect fraud,late delivery for classification type. And sales, order quantity is predicted for regression type models.\n\nA new dataset is created with the copy of original data for training the data and validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Two new columns are created for orders with suspected fraud and late delivery making them into binary classification, which in turn helps to measure performance of different models better."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['fraud'] = np.where(train_data['Order Status'] == 'SUSPECTED_FRAUD', 1, 0)\ntrain_data['late_delivery']=np.where(train_data['Delivery Status'] == 'Late delivery', 1, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to measure machine models accurately all the columns with repeated values are dropped like late_delivery_risk column because, it is known all the products with late delivery risk are delivered late. And Order Status column because, a new column for fraud detection is created there is a chance machine learning model might take values directly from these columns to predict output."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping columns with repeated values\ntrain_data.drop(['Delivery Status','Late_delivery_risk','Order Status','order_month_year','order date (DateOrders)'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is important to check the type of variables in the data because machine learning models can only be trained with numerical values."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some columns with object type data which cannot be trained in machine learning models so all the object type data is converted to int type using preprocessing label encoder library."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the Labelencoder object\nle = preprocessing.LabelEncoder()\n#convert the categorical columns into numeric\ntrain_data['Customer Country']  = le.fit_transform(train_data['Customer Country'])\ntrain_data['Market']            = le.fit_transform(train_data['Market'])\ntrain_data['Type']              = le.fit_transform(train_data['Type'])\ntrain_data['Product Name']      = le.fit_transform(train_data['Product Name'])\ntrain_data['Customer Segment']  = le.fit_transform(train_data['Customer Segment'])\ntrain_data['Customer State']    = le.fit_transform(train_data['Customer State'])\ntrain_data['Order Region']      = le.fit_transform(train_data['Order Region'])\ntrain_data['Order City']        = le.fit_transform(train_data['Order City'])\ntrain_data['Category Name']     = le.fit_transform(train_data['Category Name'])\ntrain_data['Customer City']     = le.fit_transform(train_data['Customer City'])\ntrain_data['Department Name']   = le.fit_transform(train_data['Department Name'])\ntrain_data['Order State']       = le.fit_transform(train_data['Order State'])\ntrain_data['Shipping Mode']     = le.fit_transform(train_data['Shipping Mode'])\ntrain_data['order_week_day']    = le.fit_transform(train_data['order_week_day'])\ntrain_data['Order Country']     = le.fit_transform(train_data['Order Country'])\ntrain_data['Customer Full Name']= le.fit_transform(train_data['Customer Full Name'])\n\n#display the initial records\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now all the data is transformed into int type. The dataset is split into train data and test data so model can be trained with train data and the performance of model can be evaluated using test data."},{"metadata":{},"cell_type":"markdown","source":"## Comparision of Classification Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"#All columns expect fraud\nxf=train_data.loc[:, train_data.columns != 'fraud']\n#Only fraud column\nyf=train_data['fraud']\n#Splitting the data into two parts in which 80% data will be used for training the model and 20% for testing\nxf_train, xf_test,yf_train,yf_test = train_test_split(xf,yf,test_size = 0.2,random_state = 42)\n#All columns expect fraud\nxl=train_data.loc[:, train_data.columns != 'late_delivery']\n#Only fraud column\nyl=train_data['late_delivery']\n#Splitting the data into two parts in which 80% data will be used for training the model and 20% for testing\nxl_train, xl_test,yl_train,yl_test = train_test_split(xl,yl,test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are so many different variables with different ranges standard scaler is used to standardize total the data so it is internally consistent before training the data with machine learning."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nxf_train=sc.fit_transform(xf_train)\nxf_test=sc.transform(xf_test)\nxl_train=sc.fit_transform(xl_train)\nxl_test=sc.transform(xl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is now ready to be used in machine learning models since many different models are compared training every model from begining is complicated so a function is defined to make the process bit easy. The output is in binary classification format so all the models are measured with Accuracy score,recall score and F1 score metrics. \n\nTo measure the performance of different models F1 score is used as the main metric because it is the harmonic mean of precison score and recall score.And all the scores are multiplied with 100 for better understanding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def classifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test):\n    model_f=model_f.fit(xf_train,yf_train) # Fitting train data for fraud detection\n    model_l=model_l.fit(xl_train,yl_train) # Fitting train data for predection of late delivery\n    yf_pred=model_f.predict(xf_test)\n    yl_pred=model_l.predict(xl_test)  \n    accuracy_f=accuracy_score(yf_pred, yf_test) #Accuracy for fraud detection\n    accuracy_l=accuracy_score(yl_pred, yl_test) #Accuracy for predection of late delivery\n    recall_f=recall_score(yf_pred, yf_test) #Recall score for  fraud detection\n    recall_l=recall_score(yl_pred, yl_test)# Recall score for predection of late delivery\n    conf_f=confusion_matrix(yf_test, yf_pred)# fraud detection\n    conf_l=confusion_matrix(yl_test, yl_pred)#predection of late delivery\n    f1_f=f1_score(yf_test, yf_pred)#fraud detection\n    f1_l=f1_score(yl_test, yl_pred)#predection of late delivery\n    print('Model paramters used are :',model_f)\n    print('Accuracy of fraud status is        :', (accuracy_f)*100,'%')\n    print('Recall score of fraud status is        :', (recall_f)*100,'%')\n    print('Conf Matrix of fraud status is        :\\n',  (conf_f))\n    print('F1 score of fraud status is        :', (f1_f)*100,'%')\n    print('Accuracy of late delivery status is:', (accuracy_l)*100,'%')\n    print('Recall score of late delivery status is:', (recall_l)*100,'%')\n    print('Conf Matrix of late delivery status is: \\n',(conf_l))\n    print('F1 score of late delivery status is:', (f1_l)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic classification model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = LogisticRegression(solver='lbfgs',random_state=0) #the classification model\nmodel_l = LogisticRegression(solver='lbfgs',random_state=0) #the classification model\n#Giving inputs to the defined function\nclassifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gaussian naive bayes model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = GaussianNB()\nmodel_l = GaussianNB()\nclassifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support vector machines"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = svm.LinearSVC()\nmodel_l = svm.LinearSVC()\nclassifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K nearest Neighbors classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = KNeighborsClassifier(n_neighbors=1)\nmodel_l = KNeighborsClassifier(n_neighbors=1)\nclassifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Discriminant Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = LinearDiscriminantAnalysis()\nmodel_l = LinearDiscriminantAnalysis()\nclassifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random forest classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = RandomForestClassifier()\nmodel_l = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\nclassifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra trees classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = ExtraTreesClassifier(n_estimators=100, max_depth=None,random_state=0)\nmodel_l = ExtraTreesClassifier(n_estimators=100, max_depth=None,random_state=0)\nclassifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### eXtreme Gradient Boosting Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = xgb.XGBClassifier()\nmodel_l = xgb.XGBClassifier()\nclassifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision tree classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = tree.DecisionTreeClassifier()\nmodel_l = tree.DecisionTreeClassifier()\nclassifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For better understanding and comparision of all the scores a dataframe is created"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Giving column Values\nclassification_data = {'Classification Model':  ['Logistic','Gausian Naive bayes','Support Vector Machines','K nearest Neighbour',\n                                'Linear Discriminant Analysis','Random Forest','Extra trees','eExtreme gradient boosting','Decision tree'],\n        'Accuracy Score for Fraud Detection':     [97.80,87.84,97.75,97.36,97.88,98.48,98.61,98.93,99.12],\n        'Recall Score for Fraud Detection':       [59.40,16.23,56.89,41.90,56.57,93.18,98.88,89.89,82.53],\n        'F1 Score for Fraud Detection':           [31.22,27.92,28.42,35.67,49.20,54.57,58.60,73.22,81.00],\n        'Accuracy Score for Late Delivery':       [98.84,57.27,98.84,80.82,98.37,98.60,99.17,99.24,99.37],\n        'Recall Score for Late Delivery':         [97.94,56.20,97.94,83.45,97.68,97.52,98.51,98.65,99.44],\n        'F1 Score for Late Delivery':             [98.96,71.95,98.96,82.26,98.52,98.74,99.25,99.31,99.42] }\n#Creating data frame with Column Names\nclassification_comparision = pd.DataFrame (classification_data, columns = ['Classification Model','Accuracy Score for Fraud Detection','Recall Score for Fraud Detection','F1 Score for Fraud Detection',\n                                                                           'Accuracy Score for Late Delivery','Recall Score for Late Delivery','F1 Score for Late Delivery'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparision Table for Classification Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_comparision #Printing dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_comparision.max() #Checking max values in every column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[[](http://)](http://)Considering F1 score it is clear that Decision Tree classifier is performing better for classification type with F1 score of almost 80% for fraud detection and 99.42% for late delivery.Suprisingly, all the models expect gussian model predicted the late delivery of orders with almost 98% accuracy.Just to make sure that model is predicting correctly the model is cross validated and the results are compared with accuracy of the model."},{"metadata":{},"cell_type":"markdown","source":"### Cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining cross validation model\ndef cross_validation_model(model_f,model_l,xf,yf,xl,yl):\n    model_f= model_f.fit(xf,yf)\n    model_l = model_l.fit(xl,yl)\n    scores_f = cross_val_score(model_f, xf, yf, cv=6)\n    scores_l = cross_val_score(model_l, xl, yl, cv=6)\n    print('Model used is',model_f)\n    print('Cross validation accuracy of fraud: %0.2f (+/- %0.2f)' % (scores_f.mean(), scores_f.std() * 2))\n    print('Cross validation accuracy of late : %0.2f (+/- %0.2f)' % (scores_l.mean(), scores_l.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_validation_model(model_f,model_l,xf,yf,xl,yl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, the difference between cross validated scores and accuracy scores of the model is very minimal it can be confirmed that the data is neither overfitted or underfitted, Which variable was given more importance in the model is found using feature importance method from sklearn."},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"important_col=model_f.feature_importances_.argsort()\nfeat_imp=pd.DataFrame({'Variables':xf.columns[important_col],'importance':model_f.feature_importances_[important_col]})\nfeat_imp=feat_imp.sort_values(by='importance',ascending=False)\nax = sns.catplot(x='Variables', y = 'importance', data=feat_imp, height=5, aspect=2,  kind=\"bar\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though fraud detection is not at all related to Days for shipping(real) it is very surprising to see it was given an importance of 0.12. All other important parameters like customer full name, shipping mode, type of payment used are given an importance of 0.7 which helps the company to detect fraud accurately when same customer is conducting fraud.\n\nSame way which variables were given importance for prediction of late delivery is found."},{"metadata":{"trusted":true},"cell_type":"code","source":"important_col=model_l.feature_importances_.argsort()\nfeat_imp=pd.DataFrame({'features':xl.columns[important_col],'importance':model_l.feature_importances_[important_col]})\nfeat_imp=feat_imp.sort_values(by='importance',ascending=False)\nax = sns.catplot(x='features', y = 'importance', data=feat_imp, height=5, aspect=2,  kind=\"bar\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that the columns for the days of shipping is given almost 90% importance in decision tree model,it will be interesting to see how well the model can predict when these variables are removed."},{"metadata":{},"cell_type":"markdown","source":"So a new model with the copy of train data is created\n."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data=train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping columns in new data set\nnew_data=new_data.drop(['Days for shipping (real)','Days for shipment (scheduled)'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#All columns expect fraud\nnew_xl=new_data.loc[:, new_data.columns != 'late_delivery']\n#Only fraud column\nnew_yl=train_data['late_delivery']\n#Splitting the data into two parts in which 80% data will be used for training the model and 20% for testing\nnew_xl_train, new_xl_test,new_yl_train,new_yl_test = train_test_split(new_xl,new_yl,test_size = 0.2,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standardizing data with Standardscaler module:"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_xl_train=sc.fit_transform(new_xl_train)\nnew_xl_test=sc.transform(new_xl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function for classification model is created to train one model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def New_classifiermodel(model_c,xc_train, xc_test,yc_train,yc_test):\n    model_c=model_c.fit(xc_train,yc_train)\n    yc_pred=model_c.predict(xc_test) \n    accuracy_c=accuracy_score(yc_pred, yc_test)\n    recall_c=recall_score(yc_pred, yc_test,average='weighted')\n    conf_c=confusion_matrix(yc_test, yc_pred)\n    f1_c=f1_score(yc_test, yc_pred,average='weighted')\n    print('Model paramters used are :',model_c)\n    print('Accuracy         :', (accuracy_c)*100,'%')\n    print('Recall score        :', (recall_c)*100,'%')\n    print('Conf Matrix        : \\n',(conf_c))\n    print('F1 score       :', (f1_c)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model_l=tree.DecisionTreeClassifier()\nNew_classifiermodel(new_model_l,new_xl_train, new_xl_test,new_yl_train,new_yl_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even when shipping days variables were removed the F1 score and the accuracy of the new model is nearly 84% which is still pretty good. Which variables are given more importance this time?"},{"metadata":{"trusted":true},"cell_type":"code","source":"important_col=new_model_l.feature_importances_.argsort()\nfeat_imp=pd.DataFrame({'features':new_xl.columns[important_col],'importance':new_model_l.feature_importances_[important_col]})\nfeat_imp=feat_imp.sort_values(by='importance',ascending=False)\nax = sns.catplot(x='features', y = 'importance', data=feat_imp, height=5, aspect=2,  kind=\"bar\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This time variables like shipping mode, order city,state are given more importance which helps company to use different shipping methods to deliver products faster."},{"metadata":{},"cell_type":"markdown","source":"Since Decision Tree classfier performed better for binary classification it will be interesting to see how well the model performs for multiclassification type data. So model was trained to predict order country."},{"metadata":{"trusted":true},"cell_type":"code","source":"#All columns expect order country\nxc=train_data.loc[:, train_data.columns != 'Order Country']\n#Order column country\nyc=train_data['Order Country']\n#Splitting 20% of dataset as test data \nxc_train, xc_test,yc_train,yc_test = train_test_split(xc,yc,test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xc_train=sc.fit_transform(xc_train)\nxc_test=sc.transform(xc_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Multi Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_c=tree.DecisionTreeClassifier()\nNew_classifiermodel(model_c,xc_train, xc_test,yc_train,yc_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! It is really suprising to see F1 score of almost 100% to predict order country. So it can be concluded that the Decision Tree classifier works best for this dataset for classification type."},{"metadata":{},"cell_type":"markdown","source":"Decision Tree classifier is identified as the best model in all Machine learning models for Classification Type data. How well it can perform when compared with Neural Network model?"},{"metadata":{},"cell_type":"markdown","source":"### Neural Network Model for Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"keras.layers.BatchNormalization()\nclassifier = Sequential()\n#First Hidden Layer\nclassifier.add(Dense(1024, activation='relu',kernel_initializer='random_normal', input_dim=44)) #Since we have 44 columns\n#Third Hidden Layer\nclassifier.add(Dense(512, activation='relu',kernel_initializer='random_normal'))\n#Fourth Hidden Layer\nclassifier.add(Dense(256, activation='relu',kernel_initializer='random_normal'))\n#Fifth Hidden Layer\nclassifier.add(Dense(128, activation='relu',kernel_initializer='random_normal'))\n#Sixth Hidden Layer\nclassifier.add(Dense(64, activation='relu',kernel_initializer='random_normal'))\n#Seventh Hidden Layer\nclassifier.add(Dense(32, activation='relu',kernel_initializer='random_normal'))\n#Eight Hidden Layer\nclassifier.add(Dense(16, activation='relu',kernel_initializer='random_normal'))\n#Ninth Hidden Layer\nclassifier.add(Dense(8, activation='relu',kernel_initializer='random_normal'))\n#Tenth Hidden Layer\nclassifier.add(Dense(4, activation='relu',kernel_initializer='random_normal'))\n#Eleventh Hidden Layer\nclassifier.add(Dense(2, activation='relu',kernel_initializer='random_normal'))\n#Output Layer\nclassifier.add(Dense(1, activation='sigmoid',kernel_initializer='random_normal'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since output data is binary classification the binary_crossentropy is used to measure loss and accuracy is used as metric to train the model because F1 score is not available in Keras."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is trained with batch size of 512 and 10 epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the data to the training dataset\nclassifier.fit(xf_train,yf_train, batch_size=512, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that the neural network model is performing better with every epoch even tough accuracy remained same the loss is decreasing with every epoch.Since every cell results is saved in jupyter notebook for the next iteration the model is trained with 30 more epochs and the results are displayed."},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"classifier.fit(xf_train,yf_train, batch_size=512, epochs=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is evaluated with test data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_evaluate=classifier.evaluate(xf_train, yf_train)\ntest_evaluate=classifier.evaluate(xf_test, yf_test)\nprint('accuracy for Train set is',train_evaluate)\nprint('accuracy for Test set is',test_evaluate)# evaluation of model.\nyf_pred1=classifier.predict(xf_test,batch_size=512,verbose=1)\nyf_pred=np.argmax(yf_pred1,axis=1)\nprint(f1_score(yf_test,yf_pred,average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The f1 score for neural network model is 96.48% which is pretty high and better when compared with decision tree f1 score which was 80.64.But comparing accuracy scores it can concluded that even machine learning models did pretty good for fraud detection and late delivery prediction."},{"metadata":{},"cell_type":"markdown","source":"## Comparision of Regression Models"},{"metadata":{},"cell_type":"markdown","source":"For comparison of regression models sales and order quantity are predicted"},{"metadata":{"trusted":true},"cell_type":"code","source":"xs=train_data.loc[:, train_data.columns != 'Sales']\nys=train_data['Sales']\nxs_train, xs_test,ys_train,ys_test = train_test_split(xs,ys,test_size = 0.3, random_state = 42)\nxq=train_data.loc[:, train_data.columns != 'Order Item Quantity']\nyq=train_data['Order Item Quantity']\nxq_train, xq_test,yq_train,yq_test = train_test_split(xq,yq,test_size = 0.3, random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MinMax scaler is used to standardize data since data type is regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler=MinMaxScaler()\nxs_train=scaler.fit_transform(xs_train)\nxs_test=scaler.transform(xs_test)\nxq_train=scaler.fit_transform(xq_train)\nxq_test=scaler.transform(xq_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is now ready to be used in machine learning models.Since, different models are compared here like above a function is defined. The output is regression type so accuracy cannot be used as a measure to compare different models like classification models, so all the models are compared using mean absolute error (MAE) and RMSE.\n\nThe lower the value of mean absolute error the better the model is performing and lower values of RMSE indicate better fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"def regressionmodel(model_s,model_q,xs_train, xs_test,ys_train,ys_test,xq_train, xq_test,yq_train,yq_test):\n    model_s=model_s.fit(xs_train,ys_train)#Fitting train data for sales\n    model_q=model_q.fit(xq_train,yq_train)#Fitting train data for order quantity\n    ys_pred=model_s.predict(xs_test)#predicting sales with test data\n    yq_pred=model_q.predict(xq_test)#predicting order quantity with test data\n    print('Model parameter used are:',model_s) #Printing the model to see which parameters are used\n    #Printing mean absolute error for predicting sales\n    print(\"MAE of sales is         :\", metrics.mean_absolute_error(ys_test,ys_pred))\n    #Printing Root mean squared error for predicting sales\n    print(\"RMSE of sales is        :\",np.sqrt(metrics.mean_squared_error(ys_test,ys_pred)))\n    #Printing mean absolute error for predicting order quantity\n    print(\"MAE of order quantity   :\", metrics.mean_absolute_error(yq_test,yq_pred))\n    #Printing Root mean squared error for predicting order quantity\n    print(\"RMSE of order quantity  :\",np.sqrt(metrics.mean_squared_error(yq_test,yq_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_s = linear_model.Lasso(alpha=0.1)\nmodel_q = linear_model.Lasso(alpha=0.1)\nregressionmodel(model_s,model_q,xs_train, xs_test,ys_train,ys_test,xq_train, xq_test,yq_train,yq_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_s = Ridge(alpha=1.0)\nmodel_q = Ridge(alpha=1.0)\nregressionmodel(model_s,model_q,xs_train, xs_test,ys_train,ys_test,xq_train, xq_test,yq_train,yq_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Light Gradient Boosting Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_s = lgb.LGBMRegressor()\nmodel_q = lgb.LGBMRegressor()\nregressionmodel(model_s,model_q,xs_train, xs_test,ys_train,ys_test,xq_train, xq_test,yq_train,yq_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_s = RandomForestRegressor(n_estimators=100,max_depth=10, random_state=40)\nmodel_q = RandomForestRegressor(n_estimators=100,max_depth=10, random_state=40)\nregressionmodel(model_s,model_q,xs_train, xs_test,ys_train,ys_test,xq_train, xq_test,yq_train,yq_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### eXtreme Gradient Boosting Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_s = xgb.XGBRegressor()\nmodel_q = xgb.XGBRegressor()\nregressionmodel(model_s,model_q,xs_train, xs_test,ys_train,ys_test,xq_train, xq_test,yq_train,yq_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_s = tree.DecisionTreeRegressor()\nmodel_q = tree.DecisionTreeRegressor()\nregressionmodel(model_s,model_q,xs_train, xs_test,ys_train,ys_test,xq_train, xq_test,yq_train,yq_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_s=LinearRegression()\nmodel_q=LinearRegression()\nregressionmodel(model_s,model_q,xs_train, xs_test,ys_train,ys_test,xq_train, xq_test,yq_train,yq_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For better understanding and comparision of all the scores a dataframe is created"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Giving column Values\nRegression_data = {'Regression Model':  ['Lasso','Ridge','Light Gradient Boosting','Random Forest',\n                                 'eXtreme gradient boosting','Decision tree','Linear Regression'],\n        'MAE Value for Sales' :        [1.55,0.75,0.46,0.19,0.154,0.013,0.0005],\n        'RMSE Value for Sales':        [2.33,0.97,1.66,1.79,3.13,0.918,0.0014],\n        'MAE Value for Quantity' :     [0.90,0.34,0.001,0.0001,0.0005,3.69,0.34],\n        'RMSE Value for Quantity':     [1.03,0.52,0.011,0.006,0.004,0.006,0.52] }\n#Creating data frame with Column Names\nRegression_comparision = pd.DataFrame (Regression_data, columns = ['Regression Model','MAE Value for Sales','RMSE Value for Sales',\n        'MAE Value for Quantity','RMSE Value for Quantity'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparision Table for Regression Model Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"Regression_comparision #Printing dataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MAE and RMSE values should be minimum so min function is used to find minimum vales in data frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"Regression_comparision.min()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here suprisingly, Linear regression model performed better in comparision to other models followed by decision tree regression model for predicting sales.For predicting order quantity both Random forest and eXtreme gradient boosting did very good.How well these models perform against neural network model perform to predict order quantity?\n\nThe neural network model is trained with 5 hidden layers to predict price."},{"metadata":{},"cell_type":"markdown","source":"### Neural Network Model for Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = Sequential()\n\n#First Hidden Layer\nregressor.add(Dense(512, activation='relu',kernel_initializer='normal',input_dim=44))\n#Second  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\n#Third  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\n#Fourth  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\n#Fifth  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\n\n#Output Layer\nregressor.add(Dense(1, activation='linear'))# Linear activation is used.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean absolute error is used as loss metric to train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.compile(optimizer='adam',loss='mean_absolute_error',metrics=['mean_absolute_error'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the data to the training dataset\nregressor.fit(xq_train,yq_train, batch_size=256, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is performing better with every epoch so the model is trained again with 50 more epochs"},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.fit(xq_train,yq_train, batch_size=256, epochs=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be observed that the  mean absolute error is reducing with every epoch.So it trained again with 10 more epochs to see if model can perform any better."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the data to the training dataset\nregressor.fit(xq_train,yq_train, batch_size=256, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The loss and MAE values started to increase so the training is stopped. The best MAE value observed is 0.0074. \n\nThe test data is evaluated to find the MAE, RMSE values."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_q= regressor.predict(xq_train)\npred_q_test= regressor.predict(xq_test)\nprint('MAE Value train data:',regressor.evaluate(xq_train,yq_train))\nprint('RMSE of train data:',np.sqrt(mean_squared_error(yq_train,pred_train_q)))\nprint('MAE Value test data:',regressor.evaluate(xq_test,yq_test))\nprint('RMSE of test data:',np.sqrt(mean_squared_error(yq_test,pred_q_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MAE and RMSE scores for neural network models are 0.007 and 0.022 which are pretty good. But surprisingly, the MAE and RMSE scores were lower for Random Forest and eXtreme Gradient Boosting ML models."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"After analyzing the DataCo Company dataset it has been discovered that both Western Europe and Central America are the regions with the highest number of sales but also the company lost most revenue from these regions only. And both these regions are suspected to the highest number of fraudulent transactions and orders with more late deliveries. The total sales for the company were consistent until the 2017 Quarter 3 and 10% increase in total sales by quarter and then suddenly dipped by almost 65% in 2018 quarter 1. October and November are the months with most sales in the total year. Most people preferred to do payment through debit card and all the fraud transactions are happening with wire transfer so the company should be careful when customers are using wire transfer as the company was scammed with more than 100k by a single customer. All the orders with the risk of late delivery are delivered late every time. Most of the orders with Cleats, Men's Footwear, and Women's Apparel category products are causing late delivery also these products are suspected to fraud the most.Although, the Neural Network classifier model trained for fraud detection outperformed all machine learning classifier models with an f1 score of 0.96. When compared with other classification machine learning models Decision Tree model did a good job of identifying orders with later delivery and detecting fraudulent transactions with an f1 score of 0.80. For regression type data while the Linear Regression model did better for predicting sales revenue, both Random forest and eXtreme Gradient Boosting Regression predicted the demand more accurately with MAE and RMSE scores lower than the Neural Network model. Although,the difference between the MAE,RMSE  scores of Neural Network regressor model and these ML models is very minimal. It is suprising to see Random Forest and eXtreme Gradient Boosting models outperforming Neural Network model. For further study, all the machine learning models can be compared with different datasets to confirm wheter the same machine learning models are performing better or not.And the performance of these machine learning models can be improved with hyper parameter tuning."},{"metadata":{},"cell_type":"markdown","source":"# References"},{"metadata":{},"cell_type":"markdown","source":"Ahmed, N. K., Atiya, A. F., Gayar, N. E., & El-Shishiny, H. (2010). An Empirical Comparison of Machine Learning Models for Time Series Forecasting. Econometric Reviews, 29(5-6), 594–621.\n\nbars, P., Smith, J., & Lyon, J. (2020). Python matplotlib multiple bars. Retrieved 17 April 2020, from https://stackoverflow.com/questions/14270391/python-matplotlib-multiple-bars\n\nBuilding Neural Network using Keras for Classification. (2020). Retrieved 18 April 2020, from https://medium.com/datadriveninvestor/building-neural-network-using-keras-for-classification-3a3656c726c1\n\nCarbonneau, R., Laframboise, K., & Vahidov, R.(2008). Application of machine learning techniques for supply chain demand forecasting. European Journal of Operational Research, 184(3), 1140-1154.\n\nConstante, Fabian; Silva, Fernando; Pereira, António (2019). DataCo SMART SUPPLY CHAIN FOR BIG DATA ANALYSIS, Mendeley Data, v5. Retrieved 25 March 2020, from http://dx.doi.org/10.17632/8gx2fvg2k6.5#file-5046ef5f-6df4-4ee7-9eb8-b33456b0d49e\n\nExplaining Feature Importance by example of a Random Forest. (2020). Retrieved 15 April 2020, from https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e\n\nFerreira, K.J., Lee, B.H.A., & Simchi-Levi, D.(2016). Analytics for an online retailer: Demand forecasting and price optimization. Manufacturing & Service Operations Management, 18(1), 69-88.\n\nFind Your Best Customers with Customer Segmentation in Python. (2020). Retrieved 9 April 2020, from https://towardsdatascience.com/find-your-best-customers-with-customer-segmentation-in-python-61d602f9eee6\n\nHassan, C.A., Khan M.S., & Shah, M.A.(2018). Comparison of Machine Learning Algorithms in Data classification. 24th International Conference on Automation and Computing (ICAC), Newcastle upon Tyne, United Kingdom, 2018, pp. 1-6.\n\nMartinez, A., Schmuck, C., Pereverzyev Jr, S., Pirker C., & Haltmeier, M. (2020). A machine learning framework for customer purchase prediction in the non-contractual setting. European ournal of Operational Research, 281(3), 588-596.\n\nResampling time series data with pandas – Ben Alex Keen. (2020). Retrieved 7 April 2020, from https://benalexkeen.com/resampling-time-series-data-with-pandas/\n\nRFM Segmentation | RFM Analysis, Model, Marketing & Software | Optimove. (2020). Retrieved 10 April 2020, from https://www.optimove.com/resources/learning-center/rfm-segmentation\n\nsklearn.linear_model.LinearRegression — scikit-learn 0.22.2 documentation. (2020). Retrieved 14 April 2020, from https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n\ntrees?, H., & Dixit, H. (2020). HOW TO LABEL the FEATURE IMPORTANCE with forests of trees?. Retrieved 10 April 2020, from https://stackoverflow.com/questions/37877542/how-to-label-the-feature-importance-with-forests-of-trees\n\nVakili, M., Ghamsari, M., & Rezaei, M. (2020). Performance Analysis and Comparison of Machine and Deep Learning Algorithms for IoT Data Classification. arXiv preprint arXiv:2001.09636.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}